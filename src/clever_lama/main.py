"""Main application module for CleverLama API bridge."""

import asyncio
import logging
import sys
import time
from collections.abc import AsyncIterator, Awaitable, Callable
from contextlib import asynccontextmanager
from functools import wraps
from types import MappingProxyType
from typing import Any, TypeVar

import httpx
import uvicorn
from config import settings
from constants import (
    API_VERSION,
    DEFAULT_CONTENT_TYPE,
    DEFAULT_MODEL,
    DEFAULT_SERVER_HEADER,
    HEALTH_PROBE_DELAY,
    HTTP_OK,
    HTTP_SERVER_ERROR,
    SERVER_ERROR_MESSAGE,
)
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from models import (
    ErrorResponse,
    HealthResponse,
    OllamaChatRequest,
    OllamaChatResponse,
    OllamaGenerateRequest,
    OllamaGenerateResponse,
    OllamaMessage,
    OllamaModel,
    OllamaModelsResponse,
    OllamaPullRequest,
    OllamaPullResponse,
    OllamaShowRequest,
    OllamaShowResponse,
    VersionResponse,
)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
CACHE_MODELS_KEY = 'models_cache'
CACHE_MODELS_TIMESTAMP_KEY = 'models_cache_timestamp'

# –¢–∏–ø—ã
F = TypeVar('F', bound=Callable[..., Awaitable[Any]])


class ColoredFormatter(logging.Formatter):
    """–¶–≤–µ—Ç–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –ª–æ–≥–æ–≤."""

    # ANSI escape –∫–æ–¥—ã –¥–ª—è —Ü–≤–µ—Ç–æ–≤
    COLORS = MappingProxyType(
        {
            'DEBUG': '\033[36m',  # Cyan
            'INFO': '\033[0m',  # Default/White (–≤–º–µ—Å—Ç–æ –∫—Ä–∞—Å–Ω–æ–≥–æ)
            'WARNING': '\033[33m',  # Yellow
            'ERROR': '\033[31m',  # Red (—Ç–æ–ª—å–∫–æ ERROR –±—É–¥–µ—Ç –∫—Ä–∞—Å–Ω—ã–º)
            'CRITICAL': '\033[35m',  # Magenta
            'RESET': '\033[0m',  # Reset
        }
    )

    def format(self, record: logging.LogRecord) -> str:
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        log_message = super().format(record)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        reset = self.COLORS['RESET']

        return f'{color}{log_message}{reset}'


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ü–≤–µ—Ç–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä–æ–º
def setup_logging() -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ü–≤–µ—Ç–Ω—ã–º –≤—ã–≤–æ–¥–æ–º."""
    # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä
    formatter = ColoredFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # –ü–æ–ª—É—á–∞–µ–º root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.log_level))

    # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫ root logger
    root_logger.addHandler(console_handler)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
setup_logging()
logger = logging.getLogger(__name__)

# –ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–µ–π
cache: dict[str, Any] = {}


# HTTP –∫–ª–∏–µ–Ω—Ç holder
class HTTPClientHolder:
    """–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è HTTP –∫–ª–∏–µ–Ω—Ç–∞, –∏–∑–±–µ–≥–∞–µ–º global."""

    def __init__(self) -> None:
        self.client: httpx.AsyncClient | None = None


client_holder = HTTPClientHolder()


@asynccontextmanager
async def lifespan(_app: FastAPI) -> AsyncIterator[None]:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    # Startup
    logger.info('üöÄ –ó–∞–ø—É—Å–∫ CleverLama API –º–æ—Å—Ç–∞')
    await startup_event()

    yield

    # Shutdown
    logger.info('üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ CleverLama API –º–æ—Å—Ç–∞')
    await shutdown_event()


# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title='CleverLama',
    version='1.0.0',
    description='–ú–æ—Å—Ç –º–µ–∂–¥—É Ollama API –∏ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏',
    docs_url=None,  # –û—Ç–∫–ª—é—á–∞–µ–º swagger - –∫–∞–∫ y –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ Ollama
    redoc_url=None,  # –û—Ç–∫–ª—é—á–∞–µ–º redoc
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)


@app.middleware('http')
async def add_ollama_headers(
    request: Request, call_next: Callable[[Request], Awaitable[Response]]
) -> Response:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Ollama."""
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–∫ y –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ Ollama
    response.headers['server'] = DEFAULT_SERVER_HEADER
    response.headers['content-type'] = DEFAULT_CONTENT_TYPE
    response.headers['x-process-time'] = str(process_time)

    return response


async def startup_event() -> None:
    """–°–æ–±—ã—Ç–∏–µ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    try:
        client_holder.client = httpx.AsyncClient(
            base_url=settings.api_base_url,
            headers={'Authorization': f'Bearer {settings.api_key}'},
            timeout=settings.request_timeout,
        )
        logger.info(f'‚úÖ HTTP –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {settings.api_base_url}')

        # –ü—Ä–æ–≤–æ–¥–∏–º health check –≤–Ω–µ—à–Ω–µ–≥–æ API
        await asyncio.sleep(HEALTH_PROBE_DELAY)
        await health_check_external_api()

    except Exception:
        logger.exception('‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è')
        await shutdown_event()
        raise


async def shutdown_event() -> None:
    """–°–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    if client_holder.client:
        await client_holder.client.aclose()
        logger.info('‚úÖ HTTP –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç')


def error_handler[F: Callable[..., Awaitable[Any]]](func: F) -> F:
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ endpoint'–∞—Ö."""

    @wraps(func)
    async def wrapper(*args: object, **kwargs: object) -> object:
        try:
            return await func(*args, **kwargs)
        except HTTPException:
            # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º HTTPException –∫–∞–∫ –µ—Å—Ç—å
            raise
        except Exception as e:
            logger.exception('–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ endpoint')
            raise HTTPException(
                status_code=HTTP_SERVER_ERROR,
                detail=ErrorResponse(
                    detail=f'{SERVER_ERROR_MESSAGE}: {e}',
                    error_code='INTERNAL_ERROR',
                ).model_dump(),
            ) from e

    return wrapper  # type: ignore[return-value]


async def health_check_external_api() -> None:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤–Ω–µ—à–Ω–µ–≥–æ API."""
    if not client_holder.client:
        msg = 'HTTP –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω'
        raise RuntimeError(msg)

    try:
        response = await client_holder.client.get('/models')
        if response.status_code == HTTP_OK:
            logger.info('‚úÖ –í–Ω–µ—à–Ω–∏–π API –¥–æ—Å—Ç—É–ø–µ–Ω')
        else:
            logger.warning(f'‚ö†Ô∏è –í–Ω–µ—à–Ω–∏–π API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}')
    except Exception:
        logger.exception('‚ùå –í–Ω–µ—à–Ω–∏–π API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')


def is_cache_valid() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π."""
    if CACHE_MODELS_TIMESTAMP_KEY not in cache:
        return False

    timestamp = cache[CACHE_MODELS_TIMESTAMP_KEY]
    if not isinstance(timestamp, (int, float)):
        return False

    current_time = time.time()
    cache_age_minutes = (current_time - timestamp) / 60
    return cache_age_minutes < settings.cache_duration_minutes


async def get_models_from_api() -> list[dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ API."""
    if not client_holder.client:
        msg = 'HTTP –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω'
        raise RuntimeError(msg)

    try:
        response = await client_holder.client.get('/models')
        response.raise_for_status()

        data = response.json()
        models = []

        if 'data' in data:
            for model in data['data']:
                model_id = model.get('id', DEFAULT_MODEL)
                models.append(
                    {
                        'id': model_id,
                        'object': 'model',
                        'created': str(int(time.time())),
                        'owned_by': 'aitunnel',
                    }
                )

    except Exception:
        logger.exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏–∑ API')
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        models = [
            {
                'id': DEFAULT_MODEL,
                'object': 'model',
                'created': str(int(time.time())),
                'owned_by': 'aitunnel',
            }
        ]

    return models


async def get_cached_models() -> list[dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö."""
    if is_cache_valid() and CACHE_MODELS_KEY in cache:
        models = cache[CACHE_MODELS_KEY]
        if isinstance(models, list):
            return models

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ API
    models = await get_models_from_api()
    cache[CACHE_MODELS_KEY] = models
    cache[CACHE_MODELS_TIMESTAMP_KEY] = time.time()

    return models


def create_ollama_models_response(models: list[dict[str, Any]]) -> OllamaModelsResponse:
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama –∏–∑ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π."""
    ollama_models = []

    for model in models:
        model_id = model.get('id', DEFAULT_MODEL)
        created_time_str = model.get('created', str(int(time.time())))

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º created –≤ int, –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
        try:
            created_time = (
                int(created_time_str)
                if isinstance(created_time_str, str)
                else created_time_str
            )
        except (ValueError, TypeError):
            created_time = int(time.time())

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama
        ollama_model = OllamaModel(
            name=model_id,
            model=model_id,
            modified_at=time.strftime(
                '%Y-%m-%dT%H:%M:%S.%fZ', time.gmtime(float(created_time))
            ),
            size=1000000000,  # 1GB –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            digest=f'sha256:{"0" * 64}',  # Dummy digest
            details={
                'parent_model': '',
                'format': 'gguf',
                'family': 'llama',
                'families': ['llama'],
                'parameter_size': '7B',
                'quantization_level': 'Q4_0',
            },
        )
        ollama_models.append(ollama_model)

    return OllamaModelsResponse(models=ollama_models)


def transform_messages_for_openai(
    messages: list[OllamaMessage],
) -> list[dict[str, str]]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è Ollama –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI."""
    return [{'role': msg.role, 'content': msg.content} for msg in messages]


def extract_content_from_openai_response(response_data: dict[str, Any]) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ OpenAI."""
    try:
        choices = response_data.get('choices', [])
        if choices and isinstance(choices, list):
            first_choice = choices[0]
            if isinstance(first_choice, dict):
                message = first_choice.get('message', {})
                if isinstance(message, dict):
                    content = message.get('content', '')
                    if isinstance(content, str):
                        return content
    except (IndexError, KeyError, TypeError):
        pass

    return ''


async def call_openai_api(
    messages: list[dict[str, str]], model: str, *, stream: bool = False
) -> dict[str, Any]:
    """–í—ã–∑—ã–≤–∞–µ—Ç OpenAI API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."""
    if not client_holder.client:
        msg = 'HTTP –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω'
        raise RuntimeError(msg)

    try:
        payload = {
            'model': model,
            'messages': messages,
            'stream': stream,
        }

        response = await client_holder.client.post('/chat/completions', json=payload)
        response.raise_for_status()

        return response.json()

    except httpx.HTTPStatusError as e:
        error_detail = f'OpenAI API error: {e.response.status_code}'
        try:
            error_data = e.response.json()
            if 'error' in error_data:
                error_detail = (
                    f'OpenAI API error: {error_data["error"].get("message", str(e))}'
                )
        except Exception:
            logger.exception('–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ API')

        raise HTTPException(
            status_code=e.response.status_code, detail=error_detail
        ) from e

    except Exception as e:
        logger.exception('–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI API')
        raise HTTPException(
            status_code=HTTP_SERVER_ERROR,
            detail=f'–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI API: {e}',
        ) from e


# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get('/')
@error_handler
async def health_check() -> HealthResponse:
    """Health check endpoint."""
    return HealthResponse()


@app.get('/api/version')
@error_handler
async def get_version() -> VersionResponse:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä—Å–∏—é API."""
    return VersionResponse(version=API_VERSION)


@app.get('/api/tags')
@error_handler
async def list_models() -> OllamaModelsResponse:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    models = await get_cached_models()
    return create_ollama_models_response(models)


@app.post('/api/show')
@error_handler
async def show_model(request: OllamaShowRequest) -> OllamaShowResponse:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏."""
    return OllamaShowResponse(
        license='MIT',
        modelfile=f'FROM {request.name}',
        parameters='temperature 0.7\ntop_p 0.9',
        template='{{ .Prompt }}',
        details={
            'parent_model': '',
            'format': 'gguf',
            'family': 'llama',
            'families': ['llama'],
            'parameter_size': '7B',
            'quantization_level': 'Q4_0',
        },
    )


@app.post('/api/pull')
@error_handler
async def pull_model(request: OllamaPullRequest) -> OllamaPullResponse:
    """–ò–º–∏—Ç–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏."""
    return OllamaPullResponse(
        status=f'pulling {request.name}', total=1000000000, completed=1000000000
    )


@app.post('/api/generate')
@error_handler
async def generate_text(request: OllamaGenerateRequest) -> OllamaGenerateResponse:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É—è –≤–Ω–µ—à–Ω–∏–π API."""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º prompt –≤ —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è OpenAI
    messages = [{'role': 'user', 'content': request.prompt}]

    # –í—ã–∑—ã–≤–∞–µ–º OpenAI API
    response_data = await call_openai_api(
        messages, request.model, stream=request.stream
    )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    generated_text = extract_content_from_openai_response(response_data)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama
    return OllamaGenerateResponse(
        model=request.model,
        created_at=time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
        response=generated_text,
        done=True,
        context=[],
        total_duration=1000000000,  # 1 —Å–µ–∫—É–Ω–¥–∞ –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
        load_duration=100000000,  # 0.1 —Å–µ–∫—É–Ω–¥—ã –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
        prompt_eval_count=len(request.prompt.split()),
        prompt_eval_duration=50000000,  # 0.05 —Å–µ–∫—É–Ω–¥—ã –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
        eval_count=len(generated_text.split()),
        eval_duration=900000000,  # 0.9 —Å–µ–∫—É–Ω–¥—ã –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
    )


@app.post('/api/chat')
@error_handler
async def chat_with_model(request: OllamaChatRequest) -> OllamaChatResponse:
    """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é –∏—Å–ø–æ–ª—å–∑—É—è –≤–Ω–µ—à–Ω–∏–π API."""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI
    openai_messages = transform_messages_for_openai(request.messages)

    # –í—ã–∑—ã–≤–∞–µ–º OpenAI API
    response_data = await call_openai_api(
        openai_messages, request.model, stream=request.stream
    )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    generated_text = extract_content_from_openai_response(response_data)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama
    return OllamaChatResponse(
        model=request.model,
        created_at=time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
        message=OllamaMessage(role='assistant', content=generated_text),
        done=True,
    )


if __name__ == '__main__':
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± Python
    python_version = sys.version.split()[0]
    logger.info(f'üêç Python –≤–µ—Ä—Å–∏—è: {python_version}')
    logger.info(f'‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {settings.model_dump_json(indent=2)}')

    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(
        'main:app',
        host=settings.host,
        port=settings.port,
        log_level=settings.log_level.lower(),
        access_log=True,
    )
